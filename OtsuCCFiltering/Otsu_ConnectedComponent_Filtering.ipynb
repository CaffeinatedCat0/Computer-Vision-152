{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision 152\n",
    "### Binarization, Connected Components, Filtering\n",
    "##### Catherine Shen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Problem 1: Binarization\n",
    "\n",
    "Write a Python function to implement Otsu’s method described in class. This algorithm should automatically determine an intensity level to threshold an image to segment out the foreground from the background. The output of your function should be a binary image that is black (pixel values = 0) for all background pixels and white (pixel values = 1) for all foreground pixels. Apply this function to the image can_pix.png and turn in the output image in your report.\n",
    "\n",
    "Notes:\n",
    " - Load in an image and convert it to grayscale.\n",
    " - You can use np.histogram or matplotlib.pyplot.hist to create a histogram of pixel intensities as a first step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1  \n",
    "Otsu's method produces a binary image to separate the foreground (cans) from the background. Image pixels are labeled 0 (black for background) if they are less than the calculated threshold, else they are labeled 1 (white for foreground). The threshold was calculated from the histogram data of the greyscale image. Since there are 256 levels of intensity, the number of bins was set to 256. Decreasing the number of bins would decrease the resolution of the resulting binary image.  \n",
    "  \n",
    "Greyscale image of can_pix.png  \n",
    "<img src=\"p1_greyimg.png\" />  \n",
    "  \n",
    "Histogram of image can_pix.png. The x-axis is the bins for the 256 intensity levels and the y-axis is the number of image pixels with that intensity.  \n",
    "<img src=\"p1_histo.png\" />  \n",
    "  \n",
    "Binary image of can_pix.png  \n",
    "<img src=\"p1_binimg.png\" />  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import misc\n",
    "#import imageio\n",
    "\n",
    "#Parameters: grey_img - greyscale img\n",
    "def otsu(grey_img):\n",
    "    #Total pixels\n",
    "    img_size = np.size(grey_img)\n",
    "    print('img size: ', img_size)\n",
    "    row, col = np.shape(grey_img)\n",
    "    print(str(row) + 'x' + str(col))\n",
    "    \n",
    "    binary_img = np.copy(grey_img)\n",
    "    \n",
    "    #Create new figure. Prevent overlap of figures\n",
    "    plt.figure()\n",
    "    #Typo - only 2 return values instead of 3\n",
    "    #h, bins = plt.hist(grey_img)   # Histogram of px intensities\n",
    "    #Or can use np.histogram, can ignore patches\n",
    "    #Ravel to flatten img array, return normalized histogram\n",
    "    #n, bins, patches = plt.hist(grey_img.ravel(), bins=256, density=True)\n",
    "    #Normalizing histogram did not result in total probability = 1\n",
    "    n, bins, patches = plt.hist(grey_img.ravel(), bins=256)\n",
    "    #plt.savefig('p1_histo.png')\n",
    "    #plt.show()\n",
    "    \n",
    "    #print('n: \\n', n)\n",
    "    #print('bins: \\n', bins)\n",
    "    n_length = len(n)\n",
    "    print('length n: ', n_length)\n",
    "    #b_length = len(bins)\n",
    "    print('length bins: ', len(bins))\n",
    "\n",
    "    # Set appropriate threshold using Otsu's method\n",
    "    # Your code here\n",
    "    #\n",
    "    threshold = 0\n",
    "    deviation = []\n",
    "    #Get probabilities\n",
    "    n = [n[i]/img_size for i in range(0,n_length)]\n",
    "    all_sum = np.cumsum(n)\n",
    "    #print('all_sum: \\n', all_sum)\n",
    "    all_mean = np.cumsum([j*n[j] for j in range(0,n_length)])\n",
    "    #4.Global mean\n",
    "    global_mean = all_mean[-1]\n",
    "    #Loop through each intensity level threshold, get threshold w/ max deviation\n",
    "    for i in range(0,n_length):\n",
    "        #2.Calculate the cumulative sums / weights to left and right of threshold\n",
    "        w0 = all_sum[i-1]\n",
    "        w1 = 1-w0\n",
    "        #print('w0, w1: ', w0, w1)\n",
    "        #No 0 weights \n",
    "        if w0 <= 0 or w1 <= 0:\n",
    "            deviation.append(0)\n",
    "            continue\n",
    "        #3.Calculate cumulative mean\n",
    "        mean0 = all_mean[i-1]/w0\n",
    "        mean1 = (all_mean[-1]-mean0)/w1\n",
    "        #mean0 = all_mean[i]/w1\n",
    "        #mean1 = (all_mean[-1]-mean0)/w1\n",
    "        #5.Deviation\n",
    "        temp_dev = w0*w1*(mean0-mean1)**2\n",
    "        #temp_dev = ((global_mean*w0-w1)**2)/(w0*w1)\n",
    "        deviation.append(temp_dev)\n",
    "        \"\"\"\n",
    "        if temp_dev >= deviation:\n",
    "            deviation = temp_dev\n",
    "            threshold = i\n",
    "        \"\"\"\n",
    "    #Get threshold with max deviation\n",
    "    threshold = [k for k,dev in enumerate(deviation) if dev==max(deviation)][0]\n",
    "    #print('deviation: ', deviation)\n",
    "    print('threshold: ', threshold)\n",
    "    #Output:63\n",
    "    \n",
    "    ####\n",
    "    \n",
    "    # Set pixel values in the binary_img based on the threshold\n",
    "    # Your code here\n",
    "    #\n",
    "    for i in range(0,row):\n",
    "        for j in range(0,col):\n",
    "            #Black 0 if less than threshold, else white 1\n",
    "            if binary_img[i][j] < threshold:\n",
    "                binary_img[i][j] = 0\n",
    "            else:\n",
    "                binary_img[i][j] = 1\n",
    "    return binary_img\n",
    "\n",
    "\n",
    "#Convert image to greyscale\n",
    "img = misc.imread('can_pix.png', flatten=True)\n",
    "#imshow default appears green but numpy array still correct\n",
    "#View in gray\n",
    "plt.imshow(img, cmap = 'gray')\n",
    "plt.imsave('p1_greyimg.png', img, cmap='gray')\n",
    "#plt.savefig('p1_greyimg.png')\n",
    "#print(img)\n",
    "can_bin_img = otsu(img)\n",
    "plt.figure()\n",
    "plt.imshow(can_bin_img, cmap = 'gray')\n",
    "plt.imsave('p1_binimg.png', can_bin_img, cmap='gray')\n",
    "#plt.savefig('p1_binimg.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Connected Components\n",
    "\n",
    "$\\textbf{2.1 Connected Regions}$\n",
    "\n",
    "(a) Write Python code to implement the connected component labeling algorithm discussed in class, assuming 8-connectedness. Your function should take as input a binary image (computed using your algorithm from question 1) and output a 2D matrix of the same size where each connected region is marked with a distinct positive number (e.g. 1, 2, 3). On the\n",
    "image can pix.png, display an image of detected connected components where connected region is mapped to a distinct color using imshow. You may need to handle recursive calls carefully to avoid system crash by using sys.setrecursionLimit(1000).\n",
    "\n",
    "(b) How many components do you think are in the image coins pix.jpg? What does your connected component algorithm give as output for this image? Include your output on this image in your report. It may help to reduce the size of the image before running the connected components algorithm but after converting the image to a binary image.\n",
    "\n",
    "$\\textbf{2.2 Take your own images}$\n",
    "\n",
    "For this part, you will be using images you take with your own camera. Choose three objects of different shapes, preferably with non-shiny surfaces. Possibilities include paper/cardboard cut-outs, bottle tops, pencils, Lego pieces, potatoes, etc. Take 3 pictures of these objects individually with a solid background. The object should be clearly distinguishable from the background (e.g. bright object & dark background). Include these three images in your report as in Figure 1. In addition, show similar output images as in Problem 2.1 for each of your new images (there is only 1 connected component in this case).\n",
    "\n",
    "<img src=\"sample_imgs.png\">\n",
    "<center>Figure 1: Sample images.</center>\n",
    "\n",
    "$\\textbf{2.3 Image moments and rectification}$\n",
    "\n",
    "Write three functions which compute the moments, central moments, and normalized moments of a marked region. Each function should take as input a 2D matrix (output of part 2.2) and 3 numbers $j$, $k$, and $d$. The output should be the $(j, k)$ moment $M_{j,k}$, central moment $\\mu_{j,k}$, normalized moment $m_{j,k}$ of the region marked with positive number $d$ in the input matrix.\n",
    "\n",
    "Using these functions, on each of the three images, draw the centroid of each object. Also\n",
    "compute the eigenvectors of the centralized second moment matrix and draw the two eigenvectors\n",
    "on the centroid. This should indicate the orientation of each object, see Figure 2 for the results on the example images. Turn in the outputs for your own images.\n",
    "\n",
    "<img src=\"principle_dirs.png\">\n",
    "<center>Figure 2: Sample images with principle directions overlaid.</center>\n",
    "\n",
    "$\\textbf{2.4 Image alignment}$\n",
    "\n",
    "We have seen that the orientation computed from Problem 2.3 can be used to roughly align the orientation of the region (i.e. in-plane rotation). Write a function to rotate the region around its centroid so that the eigenvector corresponding to the largest eigenvalue (i.e. the blue vector in Figure 3) will be horizontal (aligned with $[1, 0]^T$). This might look like in Figure 3.\n",
    "Your function should take as input a 2D matrix (output of Problem 2.1) and output a matrix of the same size in which all marked regions are aligned. Turn in the aligned outputs for your images.\n",
    "Note: After finding the rotation matrix R to rotate the largest eigenvector to $[1, 0]^T$ , we rotate all points $(x, y)$ belonging to that region using the following transformation\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\begin{bmatrix}x'\\\\y'\n",
    "\\end{bmatrix}=R\\begin{bmatrix}x-\\hat{x}\\\\y-\\hat{y}\n",
    "\\end{bmatrix}+\\begin{bmatrix}\\hat{x}\\\\\\hat{y}\n",
    "\\end{bmatrix}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "where $[\\hat{x}, \\hat{y}]^T$\n",
    "T are the centroid coordinates of the region. For simplicity, just ignore the cases when\n",
    "part of the aligned region falls outside of the image border or is overlapped with other regions. You\n",
    "can avoid these issues when capturing your images (e.g. put your objects a bit far apart). Finally,\n",
    "note that the rotation matrix can be created trivially from the eigenvectors.\n",
    "\n",
    "<img src=\"rotated.png\">\n",
    "<center>Figure 3: (a) Original binarized image with principal directions (b) Aligned sample image with major eigenvector aligned along $[1, 0]^T$.</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1a  \n",
    "The number of connected components of the can_pix.png image was calculated. Six different connected components were detected. Decreasing the number of bins in the histogram would decrease the resolution of the binary image, therefore, decreasing the number of detected connected components since the cans would appear to merge.  \n",
    "  \n",
    "Detected connected components of cans image  \n",
    "<img src=\"p2-1a_cc.png\" />  \n",
    "  \n",
    "## 2.1b  \n",
    "The coins_pix.png image would appear to have 12 connected components, but it actually has 2 since all but one of the coins are connected in the binary image. Decreasing the number of bins in the histogram would decrease the resolution and the number of detected connected components, while increasing the number to increase resolution would not produce much of an effect.  \n",
    "  \n",
    "The coins image was inverted so that the background is black and the foreground (coins) is light-colored to easily calculate the connected components.\n",
    "<img src=\"p2-1b_inv_coins_pix.png\" />  \n",
    "\n",
    "Binary image of coins  \n",
    "<img src=\"p2-1b_bin_coins_pix.png\" />  \n",
    "  \n",
    "Detected connected components of coins  \n",
    "<img src=\"p2-1b_cc_coins.png\" />  \n",
    "  \n",
    "## 2.2  \n",
    "Three objects with different shapes were chosen and their connected components were calculated. Using a black background with a light-colored object increases the accuracy of separating the foreground from the background.  \n",
    "  \n",
    "Tape and connected component  \n",
    "<img src=\"p2-2_tape.jpg\" />  \n",
    "<img src=\"p2-2_cctape.png\" />  \n",
    "\n",
    "Post and connected component  \n",
    "<img src=\"p2-2_post.jpg\" />  \n",
    "<img src=\"p2-2_ccpost.png\" />  \n",
    "  \n",
    "Rect and connected component  \n",
    "<img src=\"p2-2_rect.jpg\" />  \n",
    "<img src=\"p2-2_ccrect.png\" />  \n",
    "  \n",
    "## 2.3  \n",
    "Calculating the centroids and the major and minor eigenvectors and eigenvalues of each image was from calculating the moments, central moments, and normalized moments of the chosen connected component. The blue arrow is the major eigenvector that runs along the longest side of the object while the red arrow is the minor eigenvector that runs along the shortest side and is orthogonal to the major eigenvector.  \n",
    "  \n",
    "Tape  \n",
    "<img src=\"p2-3_cctape.png\" />  \n",
    "  \n",
    "Post  \n",
    "<img src=\"p2-3_ccpost.png\" />  \n",
    "  \n",
    "Rect  \n",
    "<img src=\"p2-3_ccrect.png\" />  \n",
    "  \n",
    "## 2.4  \n",
    "The chosen connected component region for each image was rotated around the centroid so that the major eigenvector is parallel to the x-axis. Some pixels in the connected component after rotation appear to be missing since each connected component pixel was rotated and rounded to the nearest pixel.  \n",
    "  \n",
    "Tape  \n",
    "<img src=\"p2-4_cctape.png\" />  \n",
    "  \n",
    "Post  \n",
    "<img src=\"p2-4_ccpost.png\" />  \n",
    "  \n",
    "Rect  \n",
    "<img src=\"p2-4_ccrect.png\" />  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import itertools\n",
    "import cv2 \n",
    "import math\n",
    "\n",
    "sys.setrecursionlimit(100000)\n",
    "\n",
    "#Parameter: img - binarized image\n",
    "def connected_component(img):\n",
    "    # your code here\n",
    "    cc_img = img.copy()\n",
    "    row, col = np.shape(cc_img)\n",
    "    #Start from 2 b/c 1 is white\n",
    "    comp = 2\n",
    "    #Label components\n",
    "    for i in range(0,row):\n",
    "        for j in range(0,col):\n",
    "            neighbors = []\n",
    "            #If pixel is 1 and not explored (explored will mark as comp)\n",
    "            if img[i][j] == 1 and cc_img[i][j] == 1:\n",
    "                #Add to neighbors and mark as explored using comp\n",
    "                neighbors.append((i,j))\n",
    "                cc_img[i][j] = comp\n",
    "                \n",
    "                #Set connected component neighbors\n",
    "                while len(neighbors) != 0:\n",
    "                    #Pop from front\n",
    "                    x, y = neighbors.pop(0)\n",
    "                    # Get 8 neighbor pixels. If not explored, add to neighbors\n",
    "                    for i in range(-1,2):\n",
    "                        for j in range(-1,2):\n",
    "                            if (x+i)>=0 and (x+i)<row and (y+j)>= 0 and (y+j)<col:\n",
    "                                #If not explored yet, add to neighbors and mark\n",
    "                                if cc_img[x+i][y+j] == 1:\n",
    "                                    neighbors.append((x+i,y+j))\n",
    "                                    cc_img[x+i][y+j] = comp\n",
    "                #label(cc_img[i][j],comp)\n",
    "                comp += 1 \n",
    "    #print('connected component: ', comp)\n",
    "    return cc_img\n",
    "\n",
    "#Parameters: cc_img - connected component img\n",
    "#            j, k - (j,k)th moment\n",
    "#            d - # of the connected region you're calculating the moment of\n",
    "def moment(cc_img, j, k, d):\n",
    "    # your code here\n",
    "    M_jk = 0\n",
    "    row, col = np.shape(cc_img)\n",
    "    for x in range(0,row):\n",
    "        for y in range(0,col):\n",
    "            #If part of desired component region\n",
    "            if cc_img[x][y] == d:\n",
    "                M_jk += (x**j)*(y**k)\n",
    "    return M_jk\n",
    "\n",
    "def central_moment(cc_img, j, k, d):\n",
    "    # your code here\n",
    "    mu_jk = 0\n",
    "    row, col = np.shape(cc_img)\n",
    "    M00 = moment(cc_img,0,0,d)\n",
    "    M01 = moment(cc_img,0,1,d)\n",
    "    M10 = moment(cc_img,1,0,d)\n",
    "    #Centroid of img\n",
    "    x_avg = M10/M00\n",
    "    y_avg = M01/M00\n",
    "    for x in range(0,row):\n",
    "        for y in range(0,col):\n",
    "            if cc_img[x][y] == d:\n",
    "                mu_jk += ((x-x_avg)**j)*((y-y_avg)**k)\n",
    "    return mu_jk\n",
    "\n",
    "def norm_moment(cc_img, j, k, d):\n",
    "    # your code here\n",
    "    m_jk = 0\n",
    "    row, col = np.shape(cc_img)\n",
    "    M00 = moment(cc_img,0,0,d)\n",
    "    M01 = moment(cc_img,0,1,d)\n",
    "    M10 = moment(cc_img,1,0,d)\n",
    "    #Centroid of img\n",
    "    x_avg = M10/M00\n",
    "    y_avg = M01/M00\n",
    "    mu20 = central_moment(cc_img,2,0,d)\n",
    "    mu02 = central_moment(cc_img,0,2,d)\n",
    "    x_sigma = math.sqrt(mu20/M00)\n",
    "    y_sigma = math.sqrt(mu02/M00)\n",
    "    for x in range(0,row):\n",
    "        for y in range(0,col):\n",
    "            if cc_img[x][y] == d:\n",
    "                m_jk += (((x-x_avg)/x_sigma)**j)*(((y-y_avg)/y_sigma)**k)\n",
    "    return m_jk\n",
    "\n",
    "#Parameters: cc_img - connected component img\n",
    "#            d - connected component #\n",
    "#            plot_align - rotation/alignment\n",
    "#            name - string for save img\n",
    "def plot_img_moments(cc_img, d, name, plot_align=0):\n",
    "    # your code here\n",
    "    row, col = np.shape(cc_img)\n",
    "    M00 = moment(cc_img,0,0,d)\n",
    "    M01 = moment(cc_img,0,1,d)\n",
    "    M10 = moment(cc_img,1,0,d)\n",
    "    #Centroid of img\n",
    "    x_avg = M10/M00\n",
    "    y_avg = M01/M00\n",
    "    centroid = np.array([[x_avg],[y_avg]])\n",
    "    print('centroid: ',centroid)\n",
    "    plt.imshow(cc_img)\n",
    "    #Plot centroid in red\n",
    "    plt.plot([y_avg],[x_avg],'ro')\n",
    "    #plt.plot([x_avg],[y_avg],'ro')\n",
    "    #Second centralized moment matrix\n",
    "    mu11 = central_moment(cc_img,1,1,d)\n",
    "    mu02 = central_moment(cc_img,0,2,d)\n",
    "    mu20 = central_moment(cc_img,2,0,d)\n",
    "    matrix_2nd = np.array([[mu20,mu11],\n",
    "                         [mu11,mu02]])\n",
    "    #Compute the major and minor eigenvectors and eigenvalues\n",
    "    e_vals, e_vectors = np.linalg.eig(matrix_2nd)\n",
    "    #print('eigenvectors: ', e_vectors)\n",
    "    #print('eigenvalues: ', e_vals)\n",
    "    #draw the two eigenvectors on the centroid\n",
    "    #find longest eigenvalue and its eigenvector\n",
    "    if(e_vals[0] < e_vals[1]):  \n",
    "        plt.arrow(y_avg,x_avg,e_vectors[1,0]*40,e_vectors[0,0]*40,shape='full',color='red',lw=3,length_includes_head=True, head_width=.01)\n",
    "        plt.arrow(y_avg,x_avg,e_vectors[1,1]*40,e_vectors[0,1]*40,shape='full',color='blue',lw=3,length_includes_head=True, head_width=.01)\n",
    "    else:\n",
    "        plt.arrow(y_avg,x_avg,e_vectors[1,0]*40,e_vectors[0,0]*40,shape='full',color='blue',lw=3,length_includes_head=True, head_width=.01)\n",
    "        plt.arrow(y_avg,x_avg,e_vectors[1,1]*40,e_vectors[0,1]*40,shape='full',color='red',lw=3,length_includes_head=True, head_width=.01)      \n",
    "    plt.title('Connected Components with two eigenvectors on centroid')\n",
    "    plt.savefig('p2-3_' + name + '.png')\n",
    "    plt.show()\n",
    "    \n",
    "    #alignment need\n",
    "    if(plot_align):\n",
    "        #longest eigenvalue and its eigenvector as column zero\n",
    "        #if(e_vals[0] < e_vals[1]):  \n",
    "        #    e_vectors_sw = e_vectors.T\n",
    "        #else:\n",
    "        #    e_vectors_sw = e_vectors\n",
    "        #aligned_img = rotate_region(cc_img, centroid, e_vectors_sw, e_vals, d)\n",
    "        aligned_img, e_vectors = rotate_region(cc_img, centroid, e_vectors, e_vals, d)\n",
    "        #plt.imshow(np.rot90(aligned_img,-3))\n",
    "        plt.imshow(aligned_img)\n",
    "        #mu20 = central_moment(aligned_img, 2, 0, d)\n",
    "        #mu02 = central_moment(aligned_img, 0, 2, d)\n",
    "        #mu11 = central_moment(aligned_img, 1, 1, d)\n",
    "        #compute eigenvectors of the centralized second moment matrix\n",
    "        #matrix_2nd = np.array([[mu20, mu11],\n",
    "        #                [mu11, mu02]])\n",
    "        #e_vals, e_vectors = np.linalg.eig(matrix_2nd)\n",
    "        #print(\"eigenvector align\", e_vectors)\n",
    "        #print(\"eigenvalue align\", e_vals)\n",
    "        #draw the two eigenvectors on the centroid\n",
    "        #the largest eigenvector corresponding to the largest eigenvalue with blue color\n",
    "        if(e_vals[0] < e_vals[1]):  \n",
    "            plt.arrow(y_avg,x_avg,e_vectors[1,0]*40,e_vectors[0,0]*40,shape='full',color='red',lw=3,length_includes_head=True, head_width=.01)\n",
    "            plt.arrow(y_avg,x_avg,e_vectors[1,1]*40,e_vectors[0,1]*40,shape='full',color='blue',lw=3,length_includes_head=True, head_width=.01)\n",
    "        else:\n",
    "            plt.arrow(y_avg,x_avg,e_vectors[1,0]*40,e_vectors[0,0]*40,shape='full',color='blue',lw=3,length_includes_head=True, head_width=.01)\n",
    "            plt.arrow(y_avg,x_avg,e_vectors[1,1]*40,e_vectors[0,1]*40,shape='full',color='red',lw=3,length_includes_head=True, head_width=.01)      \n",
    "        plt.title('Aligned Connected Component with two eigenvectors on centroid')\n",
    "        plt.savefig('p2-4_' + name + '.png')\n",
    "        plt.show()\n",
    "\n",
    "#Parameters: cc_img - connected component img\n",
    "#            centroid - 2x1 of connected component\n",
    "#            eigenvectors - 2x2\n",
    "#            e_vals - 1x2 eigenvalues\n",
    "#            d - # of the connected region you're calculating \n",
    "def rotate_region(cc_img, centroid, eigenvectors, e_vals, d):\n",
    "    #aligned_img = np.copy(cc_img)\n",
    "    # your code here\n",
    "    nrow, ncol = np.shape(cc_img)   \n",
    "    aligned_img = np.zeros((nrow,ncol))\n",
    "    x_axis = np.array([[0],[1]]).flatten()\n",
    "    #u_xaxis = x_axis/np.linalg.norm(x_axis)\n",
    "    #Normalize\n",
    "    u_xaxis = x_axis/np.sqrt(np.dot(x_axis,x_axis))\n",
    "    #[1,0].T for large eigenvector; [0,1].T for the other eigenv.\n",
    "    #I = np.eye(2)                   \n",
    "    #Angle between major eigenvector and x-axis\n",
    "    #Unit vector\n",
    "    if(e_vals[0] < e_vals[1]):  \n",
    "        #Col 1\n",
    "        e_vec1 = eigenvectors[:,1].flatten()\n",
    "    else:\n",
    "        #Col 0\n",
    "        e_vec1 = eigenvectors[:,0].flatten()\n",
    "    #Normalize\n",
    "    #u_vec1 = e_vec1/np.linalg.norm(e_vec1)\n",
    "    u_vec1 = e_vec1/np.sqrt(np.dot(e_vec1,e_vec1))\n",
    "    angle = math.acos(np.dot(u_vec1, u_xaxis))\n",
    "    #print('angle: ', angle)\n",
    "    #Rotation matrix\n",
    "    #R = np.dot(B,eigenvectors.T)    #2 x 2    \n",
    "    #R = np.dot(np.linalg.inv(eigenvectors),I) #eigenvector matrix transpose is same as invert\n",
    "    #R = np.array([[math.cos(angle),math.sin(angle)],[(-1)*(math.sin(angle)),math.cos(angle)]])\n",
    "    R = np.eye(2,2)\n",
    "    #Counter\n",
    "    R[0,0] = math.cos(angle)\n",
    "    R[0,1] = -1*math.sin(angle)\n",
    "    R[1,0] = math.sin(angle)\n",
    "    R[1,1] = math.cos(angle)\n",
    "    #print(\"expected eigenvector\", np.dot(R,eigenvectors))\n",
    "    #print(\"expected eigenvector\", np.dot(R,e_vec1))\n",
    "    #print(\"Rotate R\", R)\n",
    "    #count = 0\n",
    "    #Rotate img by angle\n",
    "    for x in range(0, nrow):\n",
    "        for y in range(0, ncol):\n",
    "            if cc_img[x,y] == d:   #marked as d #all regions or just region 1 ?\n",
    "                #count += 1\n",
    "                pix = np.array([[x],[y]])\n",
    "                aligned_pix = np.dot(R,(pix-centroid)) + centroid  #2 x 1\n",
    "                #aligned_img[int(aligned_pix[0]),int(aligned_pix[1])] = cc_img[x,y]  #i.e. 1\n",
    "                aligned_img[int(np.round(aligned_pix[0])),int(np.round(aligned_pix[1]))] = cc_img[x,y]  #i.e. 1\n",
    "                #Set as 'marked'\n",
    "                #aligned_img[x,y] = 0\n",
    "                #print(\"alinged\", x, y, aligned_pix[0], aligned_pix[1])        \n",
    "    #print(\"tot aligned pixels\", count)\n",
    "    #Rotate eigenvectors\n",
    "    eigenvectors = np.dot(R,eigenvectors)\n",
    "    return aligned_img, eigenvectors\n",
    "    \n",
    "# Part 2.1a\n",
    "#can_img = misc.imread('p1_binimg.png')\n",
    "#plt.imshow(can_bin_img)\n",
    "#plt.figure()\n",
    "#can_bin_s = cv2.resize(can_bin_img)\n",
    "cc_can = connected_component(can_bin_img)\n",
    "#print('Connected components of cans')\n",
    "#plt.imshow(cc_can)\n",
    "plt.imsave('p2-1a_cc.png',cc_can)\n",
    "\n",
    "# Part 2.1b\n",
    "plt.figure()\n",
    "img = misc.imread('coins_pix.jpg')\n",
    "#Flip every pixel to get black/dark background\n",
    "img = 1 - np.asarray(img,)\n",
    "#print('Inversed coins img')\n",
    "#plt.imshow(img)\n",
    "plt.imsave('p2-1b_inv_coins_pix.png',img)\n",
    "#Convert image to greyscale\n",
    "coin_grey = misc.imread('p2-1b_inv_coins_pix.png', flatten=True)\n",
    "coin_bin = otsu(coin_grey)\n",
    "#Make sure that histogram and bin images don't ouput with overlap\n",
    "plt.figure()\n",
    "#print('Binarized coins img')\n",
    "#plt.imshow(coin_bin, cmap='gray')\n",
    "plt.imsave('p2-1b_bin_coins_pix.png',coin_bin, cmap='gray')\n",
    "#Get connected components\n",
    "cc_coins = connected_component(coin_bin)\n",
    "#Print('Connected components of coins')\n",
    "#plt.imshow(cc_coins)\n",
    "plt.imsave('p2-1b_cc_coins.png',cc_coins)\n",
    "\n",
    "# Part 2.2\n",
    "#Tape, post-it, rectangle\n",
    "tape = misc.imread('p2-2_tape.jpg', flatten=True)\n",
    "post = misc.imread('p2-2_post.jpg', flatten=True)\n",
    "rect = misc.imread('p2-2_rect.jpg', flatten=True)\n",
    "#plt.imshow(tape)\n",
    "#plt.imshow(post)\n",
    "#plt.imshow(rect)\n",
    "tape_bin = otsu(tape)\n",
    "plt.figure()\n",
    "post_bin = otsu(post)\n",
    "plt.figure()\n",
    "rect_bin = otsu(rect)\n",
    "plt.figure()\n",
    "#Downscale img to 1/10 size\n",
    "tape_bin = cv2.resize(tape_bin, (0,0), fx=0.1, fy=0.1)\n",
    "post_bin = cv2.resize(post_bin, (0,0), fx=0.1, fy=0.1)\n",
    "rect_bin = cv2.resize(rect_bin, (0,0), fx=0.1, fy=0.1)\n",
    "cc_tape = connected_component(tape_bin)\n",
    "cc_post = connected_component(post_bin)\n",
    "cc_rect = connected_component(rect_bin)\n",
    "plt.imsave('p2-2_cctape.png', cc_tape)\n",
    "plt.imsave('p2-2_ccpost.png', cc_post)\n",
    "plt.imsave('p2-2_ccrect.png', cc_rect)\n",
    "#plt.imshow(cc_tape)\n",
    "#plt.imshow(cc_post)\n",
    "#plt.imshow(cc_rect)\n",
    "\n",
    "# Part 2.3\n",
    "plot_img_moments(cc_tape,2,name='cctape',plot_align=0)\n",
    "plot_img_moments(cc_post,2,name='ccpost',plot_align=0)\n",
    "plot_img_moments(cc_rect,6,name='ccrect',plot_align=0)\n",
    "\n",
    "# Part 2.4\n",
    "plot_img_moments(cc_tape,2,name='cctape',plot_align=1)\n",
    "plot_img_moments(cc_post,2,name='ccpost',plot_align=1)\n",
    "plot_img_moments(cc_rect,6,name='ccrect',plot_align=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Problem 3: Filtering\n",
    "\n",
    "In this problem we will play with convolution filters. Filters, when convolved with an image, will respond strongest on locations of an image that look like the filter when it is rotated by 180 degrees. This allows us to use filters as object templates in order to identify specific objects within an image. In the case of this assignment, we will be finding cars within an image by convolving a car template onto that image. Although this is not a very good way to do object detection, this problem will show you some of the steps necessary to create a good object detector. The goal of this problem will be to teach some pre-processing steps to make vision algorithms be successful and some strengths and weaknesses of filters. Each problem will ask you to analyze and explain your results. If you do not provide an explanation of why or why not something happened, then you will not get full credit.\n",
    "\n",
    "$\\textbf{3.1 Warmup - Mickey Detection}$\n",
    "\n",
    "First you will convolve a filter to a synthetic image. The filter or template is filter.jpg and the synthetic image is toy.png. These files are available on the course webpage. You will first want to modify the filter image and original slightly. We will do so by subtracting the mean of the image intensities from the image i.e. $I_t \\rightarrow I - means(\\textbf{vec}(I))$, where $I_t$ is the transformed image and $I$ is\n",
    "the original image. \n",
    "\n",
    "To convolve the filter image with the toy example, use scipy.ndimage.convolve. The output of the convolution will create an intensity image. In the original image (not the image with its mean subtracted), draw a bounding box of the same size as the filter image around the top 3 intensity value locations in the convolved image. Provide both the intensity map and bounding box images in the report.\n",
    "\n",
    "<img src=\"toy_conv.png\">\n",
    "<center>Figure 4: Example outputs for the synthetic example. Left - Intensity map. Right - Bounding boxes</center>\n",
    "\n",
    "The outputs should look like the above figure. Describe how well you think this technique will work on more realistic images. Do you foresee any problems for this algorithm on more realistic images?\n",
    "\n",
    "$\\textbf{3.2 Detection quality}$\n",
    "\n",
    "We have now created an algorithm that produces a bounding box around a detected object. However, we have no way to know if the bounding box is good or bad. In the example images shown above, the bounding boxes look reasonable, but not perfect. Given a ground truth bounding box (g) and a\n",
    "predicted bounding box (p), a commonly used measurement for bounding box quality is:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "r = \\frac{p \\cap g}{p \\cup g}\n",
    "\\end{eqnarray}\n",
    "\n",
    "More intuitively, this is the number of overlapping pixels between the bounding boxes divided by the total number of unique pixels of the two bounding boxes combined. Assuming that all bounding boxes will be axis-aligned rectangles, implement this error function and try it on the toy example in the previous section. Choose 3 different ground truth bounding box sizes around one of the Mickey 3 silhouettes. In general, if the overlap is 50% or more, you may consider that the detection did a good job.\n",
    "\n",
    "$\\textbf{3.3 Car Detection}$\n",
    "\n",
    "Now that you have created an algorithm for matching templates and a function to determine the quality of the match, it is time to try some more realistic images. The file, cartemplate.jpg , will be the filter to convolve on each of the 5 other car images $\\textbf{(car1.jpg, car2.jpg, car3.jpg)}$. Each image will have an associated text files that contains two $x, y$ coordinates (one pair per line). These coordinates will be the ground truth bounding box for each image. For each car image, provide the\n",
    "following:\n",
    "\n",
    "1. A heat map image\n",
    "2. The provided ground truth bounding box drawn on the original image (green)\n",
    "3. The detected bounding box drawn on the original image (blue)\n",
    "4. The overlap between the two bounding boxes drawn on the original image (purple)\n",
    "5. The bounding box overlap percent r × 100%\n",
    "\n",
    "<img src=\"car_conv.png\">\n",
    "<center>Figure 4: Example outputs for the car example. Left - Intensity map. Right - Bounding boxes</center>\n",
    "\n",
    "Here are some helpful hints to increase the overlap percentage:\n",
    "- Rescaling the car template to various sizes (for $\\textbf{car1.jpg}$)\n",
    "- Horizontally flipping the car template (for $\\textbf{car2.jpg}$)\n",
    "- A combination of the first two hints (for $\\textbf{car3.jpg}$)\n",
    "- Gaussian blurring might be useful in all cases\n",
    "\n",
    "An example output is shown for $\\textbf{car1.jpg}$ in Figure 5. It may not be possible to achieve 50% overlap on all the images. Your analysis of the images will be worth as much as achieving a high overlap percentage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1  \n",
    "Given a Mickey filter/template and a toy image with 3 Mickeys, the filter was convolved to the toy image to detect the 3 Mickeys, as seen in the intensity map of the toy image with the red circular areas. Bounding boxes were then placed over the areas in the toy image that have the closest matches to the filter.  \n",
    "<img src=\"p3-1_intensitymap.png\" />  \n",
    "<img src=\"p3-1_boundingboxes.png\" />   \n",
    "  \n",
    "When applied on more realistic images, the accuracy of this technique would decrease and lead to false detections since there is more noise and objects that could look similar to the filter image. \n",
    "\n",
    "## 3.2  \n",
    "Three different ground truth bounding boxes were chosen to test bounding box qualities and to determine the overlap percentage when detecting the Mickey on the upper right hand side of the toy image. In each image, the blue box is the true bounding box, the green box is for the ground truth, and the purple shows the overlap between the two boxes. All have high overlap percentages, so the detection quality is good.  \n",
    "<img src=\"p3-2_groundtruth0.png\" />  \n",
    "<img src=\"p3-2_groundtruth1.png\" />  \n",
    "<img src=\"p3-2_groundtruth2.png\" />  \n",
    "  \n",
    "## 3.3  \n",
    "A car filter/template was used to detect the cars in 3 different realistic car images. The filter image was downscaled before it was convolved to the car images so that it is more similar to the size of the car to be detected. The detected bounding box size is the same as the size of the filter used for each car image.  \n",
    "  \n",
    "Car 1 was easily detected with at least 50% overlap between the filter and the car. The color and position of the car in this image is very similar to the filter, so no further filter modifications were needed.  \n",
    "<img src=\"p3-3_intmap_car1.png\" />  \n",
    "<img src=\"p3-3_boundbox_car1.png\" />  \n",
    "  \n",
    "Car 2 required for the filter image to be flipped horizontally to match the orientation of car 2. The overlap is slightly more than 50%.  \n",
    "<img src=\"p3-3_intmap_car2.png\" />  \n",
    "<img src=\"p3-3_boundbox_car2.png\" />  \n",
    "  \n",
    "Car 3 required the most filter modifications experimentations since car 3 was difficult to detect. Car 3 is in a different color and much different orientation where mainly only the back of the car could be seen. Cropping the filter image to get the car wheels produced the most accurate detection of the tried modifications (combinations of flipping, blurring, scaling, and cropping) at around 16% since the wheels are similar in both filter and car image. Without the cropping, the detected bounding box would be placed over the tree since the color intensity and shape is roughly similar to the filter. If the filter was cropped to get the car wheels and Gaussian blurring was applied to blur the filter, the detected bounding box would move to the area on the right side of the car where the poles are in front of the black building wall since blurring would make the filter look less like wheels and more like that building wall. To increase the detection accuracy, the filter image could be inverted to match the color of car 3.  \n",
    "<img src=\"p3-3_intmap_car3.png\" />  \n",
    "<img src=\"p3-3_boundbox_car3.png\" />  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from scipy.ndimage.filters import convolve as conv\n",
    "import scipy.ndimage as ndi\n",
    "import matplotlib\n",
    "#import skimage.io \n",
    "from scipy import misc\n",
    "import copy\n",
    "import cv2\n",
    "\n",
    "\"\"\"\n",
    "def detection_quality(predicted_bbox, true_bbox):\n",
    "    # your code here\n",
    "    return r\n",
    "\"\"\"\n",
    "\n",
    "#find overlap area (%), and its bound box\n",
    "def detection_quality(predicted_bbox, true_bbox): \n",
    "    # your code here\n",
    "    a = predicted_bbox\n",
    "    b = true_bbox\n",
    "    #find overlap_area(a, b):  # returns None if rectangles don't intersect\n",
    "    dx = min(a[2], b[2]) - max(a[0], b[0])\n",
    "    dy = min(a[3], b[3]) - max(a[1], b[1])\n",
    "    if (dx>=0) and (dy>=0):\n",
    "        c = [max(a[0], b[0]), max(a[1], b[1]), min(a[2], b[2]), min(a[3], b[3])]\n",
    "        inter_area = dx*dy\n",
    "    else:\n",
    "        c = []\n",
    "        inter_area = 0\n",
    "    print(\"overlap area\", inter_area)\n",
    "    union_area = (b[2]-b[0])*(b[3]-b[1]) + (a[2]-a[0])*(a[3]-a[1]) - inter_area\n",
    "    ov_per = 100.*(inter_area / union_area)\n",
    "    print(\"union area\", union_area)\n",
    "    print(\"bounding box overlap percent\", ov_per, \"%\")  \n",
    "    return c\n",
    "\n",
    "'''\n",
    "Part 3.1\n",
    "'''\n",
    "print(\"<*Part 3.1>:\")\n",
    "#grey_image = mpimg.imread('toy.png')   #  pixels\n",
    "#image_toy = otsu(grey_image)\n",
    "#Convert filter and image to greyscale\n",
    "grey_toy = misc.imread('toy.png', flatten=True)\n",
    "image_toy = grey_toy          #no need for otsu, since it is already in binary form\n",
    "#print(\"greybinary toy\", image_toy[72,:])\n",
    "#im = Image.open('filter.jpg')\n",
    "#im.save('filter.png')          #convert jpg to png\n",
    "grey_image = misc.imread('filter.jpg', flatten=True)    # pixels\n",
    "#image_filter = otsu(grey_image)\n",
    "image_filter = grey_image       #no need for otsu, since it is already in binary form\n",
    "\n",
    "#Vectorize matrix, convert elements to float, subtract global mean for each element in filter and img\n",
    "#Toy\n",
    "row, col = np.shape(image_toy)\n",
    "#Convert matrix elements to float\n",
    "image_toy = image_toy.astype(float)\n",
    "toy_mean = np.mean(image_toy)\n",
    "#Subtract mean from every intensity pixel\n",
    "conv_float = lambda x: x-toy_mean\n",
    "float_mean = np.vectorize(conv_float)\n",
    "image_toy = float_mean(image_toy)\n",
    "\n",
    "#Filter\n",
    "row, col = np.shape(image_filter)\n",
    "#Convert matrix elements to float\n",
    "image_filter = image_filter.astype(float)\n",
    "filter_mean = np.mean(image_filter)\n",
    "#Subtract mean from every intensity pixel\n",
    "conv_float = lambda x: x-filter_mean\n",
    "float_mean = np.vectorize(conv_float)\n",
    "image_filter = float_mean(image_filter)\n",
    "        \n",
    "#Rotate filter 180 degrees\n",
    "image_filter = np.rot90(image_filter,1,(0,1))\n",
    "image_filter = np.rot90(image_filter,1,(0,1))\n",
    "plt.imshow(image_toy)\n",
    "plt.show()\n",
    "plt.imshow(image_filter)\n",
    "plt.show()\n",
    "\n",
    "#Convolve filter wih toy and get intensity map\n",
    "conv_image = ndi.convolve(image_toy, image_filter, mode='constant', cval=0.0)\n",
    "#print(\"conv toy\", conv_image[72,:])\n",
    "#plt.imshow(image_toy)    \n",
    "#plt.show()\n",
    "#plt.imshow(image_filter)    \n",
    "#plt.show()\n",
    "plt.imshow(conv_image, cmap='jet')    \n",
    "#plt.show()\n",
    "#plt.imsave('p3-1_intensitymap.png',conv_image,cmap='jet')\n",
    "\n",
    "#Plot bounding boxes around the 3 Mickey's\n",
    "heats = np.argwhere(conv_image == np.max(conv_image))\n",
    "print('heats: ', heats)\n",
    "print(\"max intensity pixel\", np.max(conv_image), np.argwhere(conv_image == np.max(conv_image)))\n",
    "#max intensity pixel -2159.0427 at [[115 307] [211 110]  [289 342]] \n",
    "#plt.plot([heats[0][1]], [heats[0][0]], 'ro')\n",
    "#plt.plot([heats[1][1]], [heats[1][0]], 'ro')\n",
    "#plt.plot([heats[2][1]], [heats[2][0]], 'ro')\n",
    "row_f,col_f = np.shape(image_filter)\n",
    "import matplotlib\n",
    "#fig = plt.figure()\n",
    "#ax = fig.add_subplot(111)\n",
    "fig,ax = plt.subplots(1)\n",
    "#ax.imshow(binary_image_toy)\n",
    "ax.imshow(grey_toy, cmap='gray')\n",
    "rect1 = matplotlib.patches.Rectangle((heats[0][1]-(col_f/2.), heats[0][0]-(row_f/2.)), col_f, row_f, edgecolor='blue', fc='none')\n",
    "rect2 = matplotlib.patches.Rectangle((heats[1][1]-(col_f/2.), heats[1][0]-(row_f/2.)), col_f, row_f, edgecolor='blue', fc='none')\n",
    "rect3 = matplotlib.patches.Rectangle((heats[2][1]-(col_f/2.), heats[2][0]-(row_f/2.)), col_f, row_f, edgecolor='blue', fc='none')\n",
    "ax.add_patch(rect1)\n",
    "ax.add_patch(rect2)\n",
    "ax.add_patch(rect3)\n",
    "plt.savefig('p3-1_boundingboxes.png')\n",
    "plt.show()\n",
    "\n",
    "'''\n",
    "Part 3.2\n",
    "'''\n",
    "print(\"<*Part 3.2>:\")\n",
    "\"\"\"\n",
    "fp = open(\"mickey1.txt\", \"r\")\n",
    "#260 40\n",
    "#400 150\n",
    "data = fp.readlines()\n",
    "t = []\n",
    "for line in data:           #file dtata read in order as 'ymin,xmin,ymax,xmax'\n",
    "    t.append(line.split())\n",
    "    #print(t)\n",
    "ymin = int(t[0][0])\n",
    "xmin = int(t[0][1])\n",
    "ymax = int(t[1][0])\n",
    "xmax = int(t[1][1])\n",
    "b = [xmin,ymin,xmax,ymax]\n",
    "\"\"\"\n",
    "b = [40,260,150,400],[30,270,150,400],[20,270,150,400]\n",
    "#Plot 3 ground truths\n",
    "for i in range(0,len(b)):\n",
    "    #pick the top right Mickey\n",
    "    fig,ax = plt.subplots(1)\n",
    "    ax.imshow(grey_toy, cmap='gray')    \n",
    "    rect1 = matplotlib.patches.Rectangle((heats[0][1]-(col_f/2.), heats[0][0]-(row_f/2.)), col_f, row_f, edgecolor='blue', fc='none')\n",
    "    ax.add_patch(rect1)\n",
    "\n",
    "    #Rectangle as 'xmin ymin xmax ymax'\n",
    "    a = [heats[0][0]-(row_f/2.), heats[0][1]-(col_f/2.), heats[0][0]+(row_f/2.), heats[0][1]+(col_f/2.)] \n",
    "    print(\"detected box\", a)  #[24.5, 276.5, 151.5, 403.5]\n",
    "\n",
    "    print(\"truth box\", b[i])\n",
    "    rectg = matplotlib.patches.Rectangle((b[i][1], b[i][0]), (b[i][3]-b[i][1]), (b[i][2]-b[i][0]), edgecolor='green', fc='none')\n",
    "    ax.add_patch(rectg)\n",
    "\n",
    "    c = detection_quality(a, b[i]) #find overlap area and detect quality in %\n",
    "    #print(\"overlap box\", c)\n",
    "    if(c != []):\n",
    "        rectv = matplotlib.patches.Rectangle((c[1], c[0]), (c[3]-c[1]), (c[2]-c[0]), edgecolor='purple', fc='none')\n",
    "        ax.add_patch(rectv)\n",
    "    plt.savefig('p3-2_groundtruth' + str(i) + '.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "'''\n",
    "Part 3.3\n",
    "'''\n",
    "print(\"<*Part 3.3>:\")\n",
    "#Convert filter to greyscale\n",
    "grey_filter = misc.imread('cartemplate.jpg', flatten=True)\n",
    "image_filter = grey_filter\n",
    "\n",
    "#Vectorize matrix, convert elements to float, subtract global mean for each element in filter \n",
    "#Filter\n",
    "row, col = np.shape(image_filter)\n",
    "#Convert matrix elements to float\n",
    "image_filter = image_filter.astype(float)\n",
    "filter_mean = np.mean(image_filter)\n",
    "#Subtract mean from every intensity pixel\n",
    "conv_float = lambda x: x-filter_mean\n",
    "float_mean = np.vectorize(conv_float)\n",
    "image_filter = float_mean(image_filter)\n",
    "        \n",
    "#Rotate filter 180 degrees\n",
    "image_filter = np.rot90(image_filter,1,(0,1))\n",
    "image_filter = np.rot90(image_filter,1,(0,1))\n",
    "plt.imshow(image_filter)\n",
    "plt.show()\n",
    "\n",
    "#3 cars\n",
    "for i in range(1,4):\n",
    "    #Downscale filter\n",
    "    if i == 1:\n",
    "        img_filter = cv2.resize(image_filter, (0,0), fx=1/8, fy=1/8)\n",
    "    #For cars 2 and 3\n",
    "    else:\n",
    "        if i == 2:\n",
    "            img_filter = cv2.resize(image_filter, (0,0), fx=1/6, fy=1/6)\n",
    "            #Flip filter horizontally\n",
    "            img_filter = np.flip(img_filter, 1)   \n",
    "        if i == 3:\n",
    "            img_filter = cv2.resize(image_filter, (0,0), fx=1/6, fy=1/6)\n",
    "            #Crop to get wheels \n",
    "            row, col = np.shape(img_filter)\n",
    "            img_filter = img_filter[:int(row*2/4),:]\n",
    "            #Apply Gaussian blur to filter for car 3\n",
    "            #img_filter = ndi.gaussian_filter(img_filter, sigma=1)\n",
    "    print('  <*Car' + str(i) + '>:')\n",
    "    #Convert car img to greyscale\n",
    "    grey_car = misc.imread('car' + str(i) + '.jpg', flatten=True)\n",
    "    image_car = grey_car\n",
    "    \n",
    "    #Vectorize matrix, convert elements to float, subtract global mean for each element in car\n",
    "    #Car\n",
    "    row, col = np.shape(image_car)\n",
    "    #Convert matrix elements to float\n",
    "    image_car = image_car.astype(float)\n",
    "    car_mean = np.mean(image_car)\n",
    "    #Subtract mean from every intensity pixel\n",
    "    conv_float = lambda x: x-car_mean\n",
    "    float_mean = np.vectorize(conv_float)\n",
    "    image_car = float_mean(image_car)\n",
    "    #plt.imshow(image_car, cmap = 'gray')\n",
    "    #plt.show()\n",
    "    \n",
    "    #Convolve filter wih car and get intensity map\n",
    "    conv_image = ndi.convolve(image_car, img_filter, mode='constant', cval=0.0)\n",
    "    #plt.imshow(image_car)    \n",
    "    #plt.show()\n",
    "    plt.imshow(img_filter)    \n",
    "    plt.show()\n",
    "    plt.imshow(conv_image, cmap='jet')    \n",
    "    #plt.show()\n",
    "    plt.imsave('p3-3_intmap_car' + str(i) + '.png',conv_image,cmap='jet')\n",
    "    \n",
    "    #Plot bounding boxes around car\n",
    "    heats = np.argwhere(conv_image == np.max(conv_image))\n",
    "    print('max intensity pixel', np.max(conv_image), np.argwhere(conv_image == np.max(conv_image)))\n",
    "    row_f,col_f = np.shape(img_filter)\n",
    "    fig,ax = plt.subplots(1)\n",
    "    ax.imshow(grey_car, cmap='gray')    \n",
    "    rect1 = matplotlib.patches.Rectangle((heats[0][1]-(col_f/2.), heats[0][0]-(row_f/2.)), col_f, row_f, edgecolor='blue', fc='none')\n",
    "    ax.add_patch(rect1)\n",
    "    #plt.show()\n",
    "    \n",
    "    #Rectangle as 'xmin ymin xmax ymax'\n",
    "    a = [heats[0][0]-(row_f/2.), heats[0][1]-(col_f/2.), heats[0][0]+(row_f/2.), heats[0][1]+(col_f/2.)] \n",
    "    print('detected box', a)  #[122.0, 252.0, 242.0, 474.0]\n",
    "    \n",
    "    fp = open('car' + str(i) + '.txt', 'r')\n",
    "    data = fp.readlines()\n",
    "    t = []\n",
    "    for line in data:\n",
    "        t.append(line.split())\n",
    "        #print(t)\n",
    "    ymin = int(t[0][0])\n",
    "    xmin = int(t[0][1])\n",
    "    ymax = int(t[1][0])\n",
    "    xmax = int(t[1][1])\n",
    "    b = [xmin,ymin,xmax,ymax]\n",
    "    print(\"truth box\", b)\n",
    "    rectg = matplotlib.patches.Rectangle((b[1], b[0]), (b[3]-b[1]), (b[2]-b[0]), edgecolor='green', fc='none')\n",
    "    ax.add_patch(rectg)\n",
    "    \n",
    "    c = detection_quality(a, b) #find overlap area and detect quality in %\n",
    "    #print(\"overlap box\", c)\n",
    "    if(c != []):\n",
    "        rectv = matplotlib.patches.Rectangle((c[1], c[0]), (c[3]-c[1]), (c[2]-c[0]), edgecolor='purple', fc='none')\n",
    "        ax.add_patch(rectv)\n",
    "    plt.savefig('p3-3_boundbox_car' + str(i) + '.png')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
